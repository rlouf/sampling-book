{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ebb553",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression\n",
    "\n",
    "In this notebook we demonstrate the use of the random walk Rosenbluth-Metropolis-Hasting algorithm on a simple logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f79edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_biclusters\n",
    "\n",
    "import blackjax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edb196",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83391a16",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "We create two clusters of points using [scikit-learn's `make_bicluster` function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_biclusters.html?highlight=bicluster%20data#sklearn.datasets.make_biclusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 50\n",
    "X, rows, cols = make_biclusters(\n",
    "    (num_points, 2), 2, noise=0.6, random_state=314, minval=-3, maxval=3\n",
    ")\n",
    "y = rows[0] * 1.0  # y[i] = whether point i belongs to cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f511b5ec",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "colors = [\"tab:red\" if el else \"tab:blue\" for el in rows[0]]\n",
    "plt.scatter(*X.T, edgecolors=colors, c=\"none\")\n",
    "plt.xlabel(r\"$X_0$\")\n",
    "plt.ylabel(r\"$X_1$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ff4b4",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "We use a simple logistic regression model to infer to which cluster each of the points belongs. We note $y$ a binary variable that indicates whether a point belongs to the first cluster :\n",
    "\n",
    "$$\n",
    "y \\sim \\operatorname{Bernoulli}(p)\n",
    "$$\n",
    "\n",
    "The probability $p$ to belong to the first cluster commes from a logistic regression:\n",
    "\n",
    "$$\n",
    "p = \\operatorname{logistic}(\\Phi\\,\\boldsymbol{w})\n",
    "$$\n",
    "\n",
    "where $w$ is a vector of weights whose priors are a normal prior centered on 0:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w} \\sim \\operatorname{Normal}(0, \\sigma)\n",
    "$$\n",
    "\n",
    "And $\\Phi$ is the matrix that contains the data, so each row $\\Phi_{i,:}$ is the vector $\\left[1, X_0^i, X_1^i\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be17b8f",
   "metadata": {
    "tags": [
     "hide-stderr"
    ]
   },
   "outputs": [],
   "source": [
    "Phi = jnp.c_[jnp.ones(num_points)[:, None], X]\n",
    "N, M = Phi.shape\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return jnp.exp(z) / (1 + jnp.exp(z))\n",
    "\n",
    "\n",
    "def log_sigmoid(z):\n",
    "    return z - jnp.log(1 + jnp.exp(z))\n",
    "\n",
    "\n",
    "def logdensity_fn(w, alpha=1.0):\n",
    "    \"\"\"The log-probability density function of the posterior distribution of the model.\"\"\"\n",
    "    log_an = log_sigmoid(Phi @ w)\n",
    "    an = Phi @ w\n",
    "    log_likelihood_term = y * log_an + (1 - y) * jnp.log(1 - sigmoid(an))\n",
    "    prior_term = alpha * w @ w / 2\n",
    "\n",
    "    return -prior_term + log_likelihood_term.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4eaca",
   "metadata": {},
   "source": [
    "## Posterior Sampling\n",
    "\n",
    "We use `blackjax`'s Random Walk RMH kernel to sample from the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = random.PRNGKey(314)\n",
    "\n",
    "w0 = random.multivariate_normal(rng_key, 0.1 + jnp.zeros(M), jnp.eye(M))\n",
    "\n",
    "rmh = blackjax.rmh(logdensity_fn, sigma=jnp.ones(M) * 0.7)\n",
    "initial_state = rmh.init(w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a5d64",
   "metadata": {},
   "source": [
    "Since `blackjax` does not provide an inference loop we need to implement one ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    @jax.jit\n",
    "    def one_step(state, rng_key):\n",
    "        state, _ = kernel(rng_key, state)\n",
    "        return state, state\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, states = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35abeef",
   "metadata": {},
   "source": [
    "We can now run the inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f07e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rng_key = random.split(rng_key)\n",
    "states = inference_loop(rng_key, rmh.step, initial_state, 5_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9a41b",
   "metadata": {},
   "source": [
    "And display the trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbad2c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "burnin = 300\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 2))\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.plot(states.position[:, i])\n",
    "    axi.set_title(f\"$w_{i}$\")\n",
    "    axi.axvline(x=burnin, c=\"tab:red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b40230",
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 300\n",
    "chains = states.position[burnin:, :]\n",
    "nsamp, _ = chains.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2a98d",
   "metadata": {},
   "source": [
    "### Predictive Distribution\n",
    "\n",
    "Having infered the posterior distribution of the regression's coefficients we can compute the probability to belong to the first cluster at each position $(X_0, X_1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ae12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meshgrid\n",
    "xmin, ymin = X.min(axis=0) - 0.1\n",
    "xmax, ymax = X.max(axis=0) + 0.1\n",
    "step = 0.1\n",
    "Xspace = jnp.mgrid[xmin:xmax:step, ymin:ymax:step]\n",
    "_, nx, ny = Xspace.shape\n",
    "\n",
    "# Compute the average probability to belong to the first cluster at each point on the meshgrid\n",
    "Phispace = jnp.concatenate([jnp.ones((1, nx, ny)), Xspace])\n",
    "Z_mcmc = sigmoid(jnp.einsum(\"mij,sm->sij\", Phispace, chains))\n",
    "Z_mcmc = Z_mcmc.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9cb5a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.contourf(*Xspace, Z_mcmc)\n",
    "plt.scatter(*X.T, c=colors)\n",
    "plt.xlabel(r\"$X_0$\")\n",
    "plt.ylabel(r\"$X_1$\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.13.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   18,
   28,
   34,
   40,
   48,
   56,
   80,
   103,
   109,
   116,
   120,
   131,
   135,
   138,
   142,
   155,
   159,
   165,
   179
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}