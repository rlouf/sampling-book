{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1cad26",
   "metadata": {},
   "source": [
    "# MLP classifier\n",
    "\n",
    "In this example we use a Multi-layer Perceptron (MLP) classifier on the MNIST digit dataset.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "We download the MNIST data using HuggingFace's `datasets` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mnist_data = load_dataset(\"mnist\")\n",
    "data_train, data_test = mnist_data[\"train\"], mnist_data[\"test\"]\n",
    "\n",
    "X_train = np.stack([np.array(example['image']) for example in data_train])\n",
    "y_train = np.array([example['label'] for example in data_train])\n",
    "\n",
    "X_test = np.stack([np.array(example['image']) for example in data_test])\n",
    "y_test = np.array([example['label'] for example in data_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa78b1",
   "metadata": {},
   "source": [
    "Now we need to apply several transformations to the dataset before splitting it into a test and a test set:\n",
    "- The images come into 28x28 pixels matrices; we reshape them into a vector;\n",
    "- The images are arrays of RGB codes between 0 and 255. We normalize them by the maximum value to get a range between 0 and 1;\n",
    "- We hot-encode category numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3133aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def one_hot_encode(x, k):\n",
    "    \"Create a one-hot encoding of x of size k.\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype=jnp.float32)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def prepare_data(X, y, num_categories=10):\n",
    "    y = one_hot_encode(y, num_categories)\n",
    "\n",
    "    num_examples = X.shape[0]\n",
    "    num_pixels = 28 * 28\n",
    "    X = X.reshape(num_examples, num_pixels)\n",
    "    X = X / 255.0\n",
    "\n",
    "    return X, y, num_examples\n",
    "\n",
    "\n",
    "def batch_data(rng_key, data, batch_size, data_size):\n",
    "    \"\"\"Return an iterator over batches of data.\"\"\"\n",
    "    while True:\n",
    "        _, rng_key = jax.random.split(rng_key)\n",
    "        idx = jax.random.choice(\n",
    "            key=rng_key, a=jnp.arange(data_size), shape=(batch_size,)\n",
    "        )\n",
    "        minibatch = tuple(elem[idx] for elem in data)\n",
    "        yield minibatch\n",
    "\n",
    "\n",
    "X_train, y_train, N_train = prepare_data(X_train, y_train)\n",
    "X_test, y_test, N_test = prepare_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ec34d",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron\n",
    "\n",
    "We will use a very simple Bayesian neural network in this example: A MLP with gaussian priors on the weights.\n",
    "\n",
    "If we note $X$ the array that represents an image and $y$ the array such that $y_i = 0$  if the image is in category $i$, $y_i=1$ otherwise, the model can be written as:\n",
    "\n",
    "```{math}\n",
    "\\begin{align*}\n",
    "  \\boldsymbol{p} &= \\operatorname{NN}(X)\\\\\n",
    "  \\boldsymbol{y} &\\sim \\operatorname{Categorical}(\\boldsymbol{p})\n",
    "\\end{align*}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee18242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax.scipy.stats as stats\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=500)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        return nn.log_softmax(x)\n",
    "\n",
    "model = NN()\n",
    "\n",
    "\n",
    "def logprior_fn(params):\n",
    "    \"\"\"Compute the value of the log-prior density function.\"\"\"\n",
    "    leaves, _ = jax.tree_util.tree_flatten(params)\n",
    "    flat_params = jnp.concatenate([jnp.ravel(a) for a in leaves])\n",
    "    return jnp.sum(stats.norm.logpdf(flat_params))\n",
    "\n",
    "\n",
    "def loglikelihood_fn(params, data):\n",
    "    \"\"\"Categorical log-likelihood\"\"\"\n",
    "    X, y = data\n",
    "    return jnp.sum(y * model.apply(params, X))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_accuracy(params, X, y):\n",
    "    \"\"\"Compute the accuracy of the model.\n",
    "\n",
    "    To make predictions we take the number that corresponds to the highest\n",
    "    probability value, which corresponds to a 1-0 loss.\n",
    "    \n",
    "    \"\"\"\n",
    "    target_class = jnp.argmax(y, axis=1)\n",
    "    predicted_class = jnp.argmax(model.apply(params, X), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd12e82",
   "metadata": {},
   "source": [
    "## Sample From the Posterior Distribution of the MLP's Weights\n",
    "\n",
    "Now we need to get initial values for the parameters, and we simply sample from their prior distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da031e26",
   "metadata": {},
   "source": [
    "We now sample from the model's posteriors using SGLD. We discard the first 1000 samples until the sampler has reached the typical set, and then take 2000 samples. We record the model's accuracy with the current values every 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e22d0",
   "metadata": {
    "tags": [
     "remove-stderr"
    ]
   },
   "outputs": [],
   "source": [
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "import blackjax\n",
    "from blackjax.sgmcmc.gradients import grad_estimator\n",
    "\n",
    "\n",
    "data_size = len(y_train)\n",
    "batch_size = 512\n",
    "step_size = 4.5e-5\n",
    "\n",
    "num_warmup = (data_size // batch_size) * 20\n",
    "num_samples = 1000\n",
    "\n",
    "# Batch the data\n",
    "rng_key = jax.random.PRNGKey(1)\n",
    "batches = batch_data(rng_key, (X_train, y_train), batch_size, data_size)\n",
    "\n",
    "# Set the initial state\n",
    "state = jax.jit(model.init)(rng_key, jnp.ones(X_train.shape[-1]))\n",
    "\n",
    "# Build the SGLD kernel with a constant learning rate\n",
    "grad_fn = grad_estimator(logprior_fn, loglikelihood_fn, data_size)\n",
    "sgld = blackjax.sgld(grad_fn)\n",
    "\n",
    "# Sample from the posterior\n",
    "accuracies = []\n",
    "steps = []\n",
    "\n",
    "pb = progress_bar(range(num_warmup))\n",
    "for step in pb:\n",
    "    _, rng_key = jax.random.split(rng_key)\n",
    "    batch = next(batches)\n",
    "    state = jax.jit(sgld)(rng_key, state, batch, step_size)\n",
    "    if step % 100 == 0:\n",
    "        accuracy = compute_accuracy(state, X_test, y_test)\n",
    "        accuracies.append(accuracy)\n",
    "        steps.append(step)\n",
    "        pb.comment = f\"| error: {100*(1-accuracy): .1f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e67bd1",
   "metadata": {},
   "source": [
    "Let us plot the point-wise accuracy at different points in the sampling process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639fce3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(steps, accuracies)\n",
    "ax.set_xlabel(\"Number of sampling steps\")\n",
    "ax.set_ylabel(\"Pointwise predictive accuracy\")\n",
    "ax.set_xlim([0, num_warmup])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_yticks([0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 1.])\n",
    "plt.title(\"Sample from 3-layer MLP posterior (MNIST dataset) with SGLD\")\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d6bc11",
   "metadata": {},
   "source": [
    "It looks like the point-wise accuracy, while still increasing, has reached a plateau. We will now sample from the posterior distribution. Instead of accumulating the network weights, which would require a subtantial amounf of memory, we will update the average of the quantity that we are interested in, the predictive probabilities over the test set.\n",
    "\n",
    "Formally, for each sample $\\theta_i$ and each $x_*$ of the test set compute $P(y_*=i\\mid x_*, \\theta_i)$. We use each sample to update the estimation of $P(y=i \\mid x_*)$ with the Monte Carlo approximation:\n",
    "\n",
    "$$\n",
    "P(y=i\\mid x_*) = \\int P(y=i\\mid x_*, \\theta)P(\\theta \\mid \\mathcal{D})\\,\\mathrm{d}\\theta \\approx \\frac{1}{N_s} \\sum_s P(y=i\\mid x_*, \\theta_s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f3cc0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_test_accuracy(i, logpredictprob, sample):\n",
    "    \"\"\"Update the running average log-predictive probability\n",
    "    and return the current value of the accuracy.\n",
    "\n",
    "    \"\"\"\n",
    "    new_logpredictprob = jnp.logaddexp(\n",
    "        logpredictprob,\n",
    "        jax.vmap(model.apply, in_axes=(None, 0))(sample, X_test)\n",
    "    )\n",
    "    predict_probs = jnp.exp(new_logpredictprob) / (i+1)\n",
    "    \n",
    "    predicted = jnp.argmax(predict_probs, axis=1)\n",
    "    target = jnp.argmax(y_test, axis=1)\n",
    "    accuracy= jnp.mean(predicted==target)\n",
    "    \n",
    "    return new_logpredictprob, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa141a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgld_accuracies = []\n",
    "sgld_logpredict = jax.vmap(model.apply, in_axes=(None, 0))(state, X_test)\n",
    "num_samples = 1000\n",
    "\n",
    "pb = progress_bar(range(num_samples))\n",
    "for step in pb:\n",
    "    _, rng_key = jax.random.split(rng_key)\n",
    "    batch = next(batches)\n",
    "    state = jax.jit(sgld)(rng_key, state, batch, step_size)\n",
    "    sgld_logpredict, accuracy = update_test_accuracy(step, sgld_logpredict, state)\n",
    "    sgld_accuracies.append(accuracy)\n",
    "    pb.comment = f\"| avg error: {100*(1-accuracy): .1f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75893e8b",
   "metadata": {},
   "source": [
    "Let us plot the accuracy as a function of the number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47156b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(range(num_samples), sgld_accuracies)\n",
    "ax.set_xlabel(\"Number of sampling steps\")\n",
    "ax.set_ylabel(\"Running average predictive accuracy\")\n",
    "ax.set_xlim([0, num_samples])\n",
    "plt.title(\"Sample from 3-layer MLP posterior (MNIST dataset) with SGLD\")\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7b320",
   "metadata": {},
   "source": [
    "It is not clear from the figure above whether the increase of the accuracy is due to an increase in the pointwise accuracy, or an effect of averaging over the posterior distribution. To see this, let us compare the last value to the pointwise accuracy computed on the chain's last state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_accuracy = compute_accuracy(state, X_test, y_test)\n",
    "print(sgld_accuracies[-1], last_accuracy)\n",
    "print(sgld_accuracies[-1] - last_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55161b8",
   "metadata": {},
   "source": [
    "Averaging the predictive probabilities over the posterior distribution leads to a decrease of 0.8 error point compared to the point-wise accuracy. And in the end, this leads to a decent accuracy for a model that was not fine-tuned (we took the first value of the step size that led to an increasing accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42afa2",
   "metadata": {},
   "source": [
    "### Sampling with SGHMC\n",
    "\n",
    "We can also use SGHMC with a constant learning rate to samples from this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcdb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 4.5e-6\n",
    "num_warmup = (data_size // batch_size) * 20\n",
    "\n",
    "grad_fn = grad_estimator(logprior_fn, loglikelihood_fn, data_size)\n",
    "sghmc = blackjax.sghmc(grad_fn, num_integration_steps=10)\n",
    "\n",
    "\n",
    "# Batch the data\n",
    "state = jax.jit(model.init)(rng_key, jnp.ones(X_train.shape[-1]))\n",
    "\n",
    "# Sample from the posterior\n",
    "sghmc_accuracies = []\n",
    "samples = []\n",
    "steps = []\n",
    "\n",
    "pb = progress_bar(range(num_warmup))\n",
    "for step in pb:\n",
    "    _, rng_key = jax.random.split(rng_key)\n",
    "    minibatch = next(batches)\n",
    "    state = jax.jit(sghmc)(rng_key, state, minibatch, step_size)\n",
    "    if step % 100 == 0:\n",
    "        sghmc_accuracy = compute_accuracy(state, X_test, y_test)\n",
    "        sghmc_accuracies.append(sghmc_accuracy)\n",
    "        steps.append(step)\n",
    "        pb.comment = f\"| error: {100*(1-sghmc_accuracy): .1f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c8c069",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(steps, sghmc_accuracies)\n",
    "ax.set_xlabel(\"Number of sampling steps\")\n",
    "ax.set_ylabel(\"Pointwise predictive accuracy\")\n",
    "ax.set_xlim([0, num_warmup])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_yticks([0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 1.])\n",
    "plt.title(\"Sample from 3-layer MLP posterior (MNIST dataset) with SGHMC\")\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fcea9",
   "metadata": {},
   "source": [
    "We now sample and compute the accuracy by averaging over the posterior samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sghmc_accuracies = []\n",
    "sghmc_logpredict = jax.vmap(model.apply, in_axes=(None, 0))(state, X_test)\n",
    "\n",
    "pb = progress_bar(range(num_samples))\n",
    "for step in pb:\n",
    "    _, rng_key = jax.random.split(rng_key)\n",
    "    batch = next(batches)\n",
    "    state = jax.jit(sgld)(rng_key, state, batch, step_size)\n",
    "    sghmc_logpredict, accuracy = update_test_accuracy(step, sghmc_logpredict, state)\n",
    "    sghmc_accuracies.append(accuracy)\n",
    "    pb.comment = f\"| avg error: {100*(1-accuracy): .1f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0fbb4",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(range(num_samples), sghmc_accuracies)\n",
    "ax.set_xlabel(\"Number of sampling steps\")\n",
    "ax.set_ylabel(\"Running average predictive accuracy\")\n",
    "ax.set_xlim([0, num_samples])\n",
    "plt.title(\"Sample from 3-layer MLP posterior (MNIST dataset) with SGLD\")\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3acb2",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Let us plot the evolution of the accuracy as a function of the number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea96e5e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(range(num_samples), sgld_accuracies, label=\"SGLD\")\n",
    "ax.plot(range(num_samples), sghmc_accuracies, label=\"SGHMC\")\n",
    "ax.set_xlabel(\"Number of sampling steps\")\n",
    "ax.set_ylabel(\"Running average predictive accuracy\")\n",
    "ax.set_xlim([0, num_samples])\n",
    "plt.title(\"Sample from 3-layer MLP posterior (MNIST dataset)\")\n",
    "plt.legend()\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941c5b0",
   "metadata": {},
   "source": [
    "SGHMC gives a slightly better accuracy than SGLD. However, plotting this in terms of the number of steps is slightly misleading: SGHMC evaluates the gradient 10 times for each step while SGLD only once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79765f2f",
   "metadata": {},
   "source": [
    "## Exploring uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c36f8",
   "metadata": {},
   "source": [
    "Let us now use the average posterior predictive probabilities to see whether the model is overconfident. Here we will say that the model is unsure of its prediction for a given image if the digit that is most often predicted for this image is predicted less tham 95% of the time.\n",
    "\n",
    "We will use SGHMC's prediction in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b19ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_probs = jnp.exp(sghmc_logpredict) / num_samples\n",
    "max_predict_prob = jnp.max(predict_probs, axis=1)\n",
    "predicted = jnp.argmax(predict_probs, axis=1)\n",
    "\n",
    "certain_mask = max_predict_prob > 0.95\n",
    "print(f\"Our model is certain of its classification for {np.sum(certain_mask) / y_test.shape[0] * 100:.1f}% of the test set examples.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83264b31",
   "metadata": {},
   "source": [
    "Let's plot a few examples where the model was very uncertain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39600f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_uncertain_idx = np.argsort(max_predict_prob)\n",
    "\n",
    "for i in range(10):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.imshow(X_test[most_uncertain_idx[i]].reshape(28, 28), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b890bb",
   "metadata": {},
   "source": [
    "Are there digits that our model is more uncertain about? We plot the histogram of the number of times the model was unsure about each digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b61a23",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "uncertain_mask = max_predict_prob < 0.95\n",
    "\n",
    "ax.bar(np.arange(10), np.bincount(np.argmax(y_test[uncertain_mask], axis=1)))\n",
    "ax.set_xticks(range(0,10))\n",
    "ax.set_xlabel(\"Digit\")\n",
    "ax.set_ylabel(\"# uncertain predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c468369",
   "metadata": {},
   "source": [
    "Perhaps unsurprisingly, the digit 8 is overrepresented in the set of examples $i$ for which $\\max_d P(y_i=d|x_i) < 0.95$. As a purely academic exercise and sanity test of sort, let us now re-compute the point-wise accuracy ignoring the digits for which the model is uncertain, varying the threshold above which we consider the model to be certain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995fa9bd",
   "metadata": {
    "args": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(probs, y):\n",
    "    predicted = jnp.argmax(probs, axis=1)\n",
    "    target = jnp.argmax(y, axis=1)\n",
    "    accuracy= jnp.mean(predicted==target)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8314916a",
   "metadata": {
    "args": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.1, 1., 90)\n",
    "\n",
    "accuracies = []\n",
    "dropped_ratio = []\n",
    "for t in thresholds:\n",
    "    certain_mask = max_predict_prob >= t\n",
    "    dropped_ratio.append(100*(1 - np.sum(certain_mask) / np.shape(certain_mask)[0]))\n",
    "    accuracies.append(compute_accuracy(predict_probs[certain_mask], y_test[certain_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df337f",
   "metadata": {
    "args": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(thresholds, accuracies)\n",
    "ax.set(xlabel=\"Threshold\", ylabel=\"Accuracy\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(thresholds, dropped_ratio)\n",
    "ax.set(xlabel=\"Threshold\", ylabel=\"% of examples dropped\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e8e825",
   "metadata": {},
   "source": [
    "Not bad at all, by dropping less than 2% of the samples we reach .99 accuracy, not too bad for such a simple model!\n",
    "\n",
    "Such a simple rejection criterion may not be realistic in practice. But what Bayesian methods allow you to do is to design a *loss function* that describe the cost of each mistake (say choosing \"1\" when the digit was in fact \"9\"), and integrating this function over your posterior allows you to make principled decisions about which digit to predict for each example."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.13.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   24,
   37,
   44,
   79,
   94,
   134,
   140,
   144,
   185,
   189,
   205,
   215,
   237,
   250,
   254,
   267,
   271,
   275,
   279,
   285,
   313,
   328,
   332,
   346,
   359,
   365,
   380,
   384,
   388,
   394,
   401,
   405,
   414,
   418,
   430,
   434,
   444,
   457,
   474
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}