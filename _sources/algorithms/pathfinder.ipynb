{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7a637c",
   "metadata": {},
   "source": [
    "# Pathfinder\n",
    "\n",
    "In this notebook we introduce the pathfinder algorithm and we show how to use it as a variational inference method or as an initialization tool for MCMC kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.datasets import make_biclusters\n",
    "\n",
    "import blackjax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d20ae",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1df54d",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "We create two clusters of points using [scikit-learn's `make_bicluster` function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_biclusters.html?highlight=bicluster%20data#sklearn.datasets.make_biclusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 50\n",
    "X, rows, cols = make_biclusters(\n",
    "    (num_points, 2), 2, noise=0.6, random_state=314, minval=-3, maxval=3\n",
    ")\n",
    "y = rows[0] * 1.0  # y[i] = whether point i belongs to cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfab55b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "colors = [\"tab:red\" if el else \"tab:blue\" for el in rows[0]]\n",
    "plt.scatter(*X.T, edgecolors=colors, c=\"none\")\n",
    "plt.xlabel(r\"$X_0$\")\n",
    "plt.ylabel(r\"$X_1$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b5009",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "We use a simple logistic regression model to infer to which cluster each of the points belongs. We note $y$ a binary variable that indicates whether a point belongs to the first cluster:\n",
    "\n",
    "$$\n",
    "y \\sim \\operatorname{Bernoulli}(p)\n",
    "$$\n",
    "\n",
    "The probability $p$ to belong to the first cluster commes from a logistic regression:\n",
    "\n",
    "$$\n",
    "p = \\operatorname{logistic}(\\Phi\\,\\boldsymbol{w})\n",
    "$$\n",
    "\n",
    "where $w$ is a vector of weights whose priors are a normal prior centered on 0:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w} \\sim \\operatorname{Normal}(0, \\sigma)\n",
    "$$\n",
    "\n",
    "And $\\Phi$ is the matrix that contains the data, so each row $\\Phi_{i,:}$ is the vector $\\left[X_0^i, X_1^i\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = X\n",
    "N, M = Phi.shape\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return jnp.exp(z) / (1 + jnp.exp(z))\n",
    "\n",
    "\n",
    "def log_sigmoid(z):\n",
    "    return z - jnp.log(1 + jnp.exp(z))\n",
    "\n",
    "\n",
    "def logdensity_fn(w, alpha=1.0):\n",
    "    \"\"\"The log-probability density function of the posterior distribution of the model.\"\"\"\n",
    "    log_an = log_sigmoid(Phi @ w)\n",
    "    an = Phi @ w\n",
    "    log_likelihood_term = y * log_an + (1 - y) * jnp.log(1 - sigmoid(an))\n",
    "    prior_term = alpha * w @ w / 2\n",
    "\n",
    "    return -prior_term + log_likelihood_term.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0aab68",
   "metadata": {},
   "source": [
    "## Pathfinder: Parallel Quasi-Newton Variational Inference\n",
    "\n",
    "Starting from a random initialization, Pathfinder locates normal approximations to the target\n",
    "density along a quasi-Newton optimization path, with local covariance estimated using the inverse Hessian\n",
    "estimates produced by the optimizer. Pathfinder returns draws from the approximation with the lowest\n",
    "estimated Kullback-Leibler (KL) divergence to the true posterior.\n",
    "The optimizer is the limited memory BFGS algorithm.\n",
    "\n",
    "To help understand the approximations that pathfinder evaluates during its run, here we plot for each step of the L-BFGS optimizer the approximation of the posterior distribution of the model derived by pathfinder and its ELBO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c07eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = random.PRNGKey(314)\n",
    "w0 = random.multivariate_normal(rng_key, 2.0 + jnp.zeros(M), jnp.eye(M))\n",
    "_, info = blackjax.vi.pathfinder.approximate(rng_key, logdensity_fn, w0, ftol=1e-4)\n",
    "path = info.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef3036",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def ellipse_confidence(mu, cov, ax, c, n_std=2.0):\n",
    "    import numpy as np\n",
    "\n",
    "    lambda_, v = np.linalg.eig(cov)\n",
    "    lambda_ = np.sqrt(lambda_)\n",
    "    ellipse = Ellipse(\n",
    "        xy=(*mu,),\n",
    "        width=lambda_[0] * n_std * 2,\n",
    "        height=lambda_[1] * n_std * 2,\n",
    "        angle=np.degrees(np.arctan2(*v[:, 0][::-1])),\n",
    "        facecolor=c,\n",
    "        edgecolor=\"b\",\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    return ax.add_artist(ellipse)\n",
    "\n",
    "\n",
    "step = 0.1\n",
    "x_, y_ = jnp.mgrid[-1:3:step, -1:3:step]\n",
    "pos_ = jnp.dstack((x_, y_))\n",
    "logp_ = jnp.nan_to_num(\n",
    "    jax.vmap(logdensity_fn)(pos_.reshape(-1, M)).reshape(pos_.shape[0], pos_.shape[1]),\n",
    "    nan=-1e10,\n",
    ")\n",
    "levels_ = jnp.percentile(logp_.flatten(), jnp.linspace(60, 100, 10))\n",
    "\n",
    "\n",
    "steps = (jnp.isfinite(path.elbo)).sum()\n",
    "rows = int(jnp.ceil(steps / 3))\n",
    "fig, axs = plt.subplots(rows, 3, figsize=(15, 5 * rows), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in zip(range(1, steps + 1), axs.flatten()):\n",
    "\n",
    "    ax.contour(x_, y_, logp_, levels=levels_)\n",
    "    state = jax.tree_map(lambda x: x[i], path)\n",
    "    sample_state, _ = blackjax.vi.pathfinder.sample(rng_key, state, 10_000)\n",
    "    position_path = path.position[: i + 1]\n",
    "    ax.plot(\n",
    "        position_path[:, 0],\n",
    "        position_path[:, 1],\n",
    "        marker=\"*\",\n",
    "        linestyle=\"--\",\n",
    "        markersize=10,\n",
    "    )\n",
    "    mu_i, cov_i = sample_state.mean(0), jnp.cov(sample_state, rowvar=False)\n",
    "    ellipse_confidence(mu_i, cov_i, ax, \"r\")\n",
    "    ax.set_title(f\"Iteration: {i+1}\\nEstimated ELBO: {state.elbo:.2f}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa51154",
   "metadata": {},
   "source": [
    "## Pathfinder as a Variational Inference Method\n",
    "\n",
    "Pathfinder can be used as a variational inference method, using its kernel API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b29bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathfinder = blackjax.kernels.pathfinder(logdensity_fn)\n",
    "state, _ = pathfinder.approximate(rng_key, w0, ftol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4761b6e",
   "metadata": {},
   "source": [
    "We can now get samples from the approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rng_key = random.split(rng_key)\n",
    "samples, _ = pathfinder.sample(rng_key, state, 5_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b581b27",
   "metadata": {},
   "source": [
    "And display the trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8f20d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 2), sharey=True)\n",
    "for i, axi in enumerate(ax):\n",
    "    axi.plot(samples[:, i])\n",
    "    axi.set_title(f\"$w_{i}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3a466",
   "metadata": {},
   "source": [
    "Please note that pathfinder is implemented as follows:\n",
    "\n",
    "* it runs L-BFGS optimization and finds the best approximation in the `init` phase\n",
    "* `step` phase it's just sampling from a multinormal distribution, whose parameters have been already estimated\n",
    "\n",
    "hence it makes sense to `jit` the `init` function and then use the `blackjax.vi.pathfinder.sample_from_state` helper function instead of implementing the inference loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21612f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "state, _ = jax.jit(pathfinder.approximate)(rng_key, w0)\n",
    "samples, _ = pathfinder.sample(rng_key, state, 5_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284911ea",
   "metadata": {},
   "source": [
    "Quick comparison against `rmh` kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop(rng_key, kernel, initial_state, num_samples):\n",
    "    @jax.jit\n",
    "    def one_step(state, rng_key):\n",
    "        state, info = kernel(rng_key, state)\n",
    "        return state, (state, info)\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    return jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "rmh = blackjax.kernels.rmh(logdensity_fn, sigma=jnp.ones(M) * 0.7)\n",
    "state_rmh = rmh.init(w0)\n",
    "_, (samples_rmh, _) = inference_loop(rng_key, rmh.step, state_rmh, 5_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798274d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 4), sharey=True)\n",
    "for i in range(2):\n",
    "    ax[i, 0].plot(samples_rmh.position[:, i])\n",
    "    ax[i, 0].axvline(x=300, c=\"tab:red\")\n",
    "    ax[i, 0].set_ylabel(f\"$w_{i}$\")\n",
    "    ax[i, 1].plot(samples[:, i])\n",
    "\n",
    "ax[0, 0].set_title(\"RMH\")\n",
    "ax[0, 1].set_title(\"Pathfinder\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321abc5a",
   "metadata": {},
   "source": [
    "### Pathfinder as an Initialization Tool for MCMC Kernels\n",
    "\n",
    "Pathfinder uses internally the inverse hessian estimation of the L-BFGS optimizer to evaluate the approximations to the target distribution along the quasi-Newton optimization path.\n",
    "\n",
    "We can calculate explicitly this inverse hessian matrix for a step of the optimization path using the `blackjax.vi.pathfinder.lbfgs_inverse_hessian_formula_1` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b7f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blackjax.optimizers.lbfgs import lbfgs_inverse_hessian_formula_1\n",
    "\n",
    "inverse_mass_matrix = lbfgs_inverse_hessian_formula_1(\n",
    "    state.alpha, state.beta, state.gamma\n",
    ")\n",
    "inverse_mass_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b37fe",
   "metadata": {},
   "source": [
    "This estimation of the inverse mass matrix, coupled with Nesterov's dual averaging adaptation for estimating the step size, yields an alternative adaptation scheme for initializing MCMC kernels.\n",
    "\n",
    "This scheme is implemented in `blackjax.kernel.pathfinder_adaptation` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce767861",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt = blackjax.kernels.pathfinder_adaptation(blackjax.nuts, logdensity_fn)\n",
    "(state, parameters), info = adapt.run(rng_key, w0, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256d503",
   "metadata": {},
   "source": [
    "## Some Caveats\n",
    "\n",
    "* L-BFGS algorithm struggles with float32s and log-likelihood functions; it's suggested to use double precision numbers. In order to do that in `jax` a configuration variable needs to be set up at initialization time (see [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#double-64bit-precision))\n",
    "\n",
    "* otherwise you can stick with float32 mode and try to tweak `ftol`, `gtol`, or the initialization point\n",
    "\n",
    "* It may make sense to start pathfinder with a \"bad\" initialization point, in order to make the L-BFGS algorithm run longer and have more datapoints to estimate the inverse hessian matrix."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.13.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   18,
   29,
   35,
   41,
   49,
   57,
   81,
   102,
   114,
   121,
   172,
   178,
   181,
   185,
   188,
   192,
   200,
   209,
   214,
   218,
   233,
   246,
   254,
   261,
   267,
   270
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}