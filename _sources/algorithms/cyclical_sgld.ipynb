{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9325769",
   "metadata": {},
   "source": [
    "# Cyclical SGLD\n",
    "\n",
    "In this example we will demonstrate how Blackjax can be used to create non-trivial samplers by implementing Cyclical SGLD {cite:p}`zhang2019cyclical`. Stochastic Gradient MCMC algorithms are typically used to sample from the posterior distribution of Bayesian Neural Networks. They differ from other gradient-based MCMC algorithms in that they estimate the gradient with minibatches of data instead of the full dataset.\n",
    "\n",
    "However, SGMCMC algorithms are inefficient at exploring multimodal distributions which are typical of neural networks. To see this let's consider a simple yet challenging example, an array of 25 gaussian distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aeba49",
   "metadata": {
    "tags": [
     "remove-stdout"
    ]
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import jax\n",
    "import jax.scipy as jsp\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "lmbda = 1/25\n",
    "positions = [-4, -2, 0, 2, 4]\n",
    "mu = jnp.array([list(prod) for prod in itertools.product(positions, positions)])\n",
    "sigma = 0.03 * jnp.eye(2)\n",
    "\n",
    "def logprob_fn(x, *_):\n",
    "    return lmbda * jsp.special.logsumexp(jax.scipy.stats.multivariate_normal.logpdf(x, mu, sigma))\n",
    "\n",
    "def sample_fn(rng_key):\n",
    "    choose_key, sample_key = jax.random.split(rng_key)\n",
    "    samples = jax.random.multivariate_normal(sample_key, mu, sigma)\n",
    "    return jax.random.choice(choose_key, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c33b6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "samples = jax.vmap(sample_fn)(jax.random.split(rng_key, 10_000))\n",
    "\n",
    "xmin, ymin = -5, -5\n",
    "xmax, ymax = 5, 5\n",
    "\n",
    "nbins = 300j\n",
    "x, y = samples[:, 0], samples[:, 1]\n",
    "xx, yy = np.mgrid[xmin:xmax:nbins, ymin:ymax:nbins]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "values = np.vstack([x, y])\n",
    "kernel = gaussian_kde(values)\n",
    "f = np.reshape(kernel(positions).T, xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cfset = ax.contourf(xx, yy, f, cmap='Blues')\n",
    "ax.imshow(np.rot90(f), cmap='Blues', extent=[xmin, xmax, ymin, ymax])\n",
    "cset = ax.contour(xx, yy, f, colors='k')\n",
    "\n",
    "plt.rcParams['axes.titlepad'] = 15.\n",
    "plt.title(\"Samples from a mixture of 25 normal distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650fbfb1",
   "metadata": {},
   "source": [
    "## SGLD\n",
    "\n",
    "Let us build a SGLD sampler with Blackjax with a decreading learning rate, and generate samples from this distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjax\n",
    "import jax\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "\n",
    "# 50k iterations\n",
    "num_training_steps = 50000\n",
    "schedule_fn = lambda k: 0.05 * k ** (-0.55)\n",
    "schedule = [schedule_fn(i) for i in range(1, num_training_steps+1)]\n",
    "\n",
    "grad_fn = lambda x, _: jax.grad(logprob_fn)(x)\n",
    "sgld = blackjax.sgld(grad_fn)\n",
    "\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "init_position = -10 + 20 * jax.random.uniform(rng_key, shape=(2,))\n",
    "\n",
    "position = init_position\n",
    "sgld_samples = []\n",
    "for i in progress_bar(range(num_training_steps)):\n",
    "    _, rng_key = jax.random.split(rng_key)\n",
    "    position = jax.jit(sgld)(rng_key, position, 0, schedule[i])\n",
    "    sgld_samples.append(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f219378",
   "metadata": {},
   "source": [
    "As one can see on the following figure, SGLD has a hard time escaping the mode in which it started, leading to a poor approximation of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1760fea",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = [sample[0] for sample in sgld_samples]\n",
    "y = [sample[1] for sample in sgld_samples]\n",
    "\n",
    "ax.plot(x, y, 'k-', lw=0.1, alpha=0.5)\n",
    "ax.set_xlim([-8, 8])\n",
    "ax.set_ylim([-8, 8])\n",
    "\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b71f8",
   "metadata": {},
   "source": [
    "## Cyclical SGLD\n",
    "\n",
    "To escape modes and better explore distributions, Cyclical SLGD alternes between two phases:\n",
    "\n",
    "1. *Exploration* using Stochastic Gradient Descent with a large-ish step size;\n",
    "2. *Sampling* using SGLD with a lower learning rate.\n",
    "\n",
    "### Cyclical schedule\n",
    "\n",
    "Both the step size and the phase the algorithm is in are governed by a cyclical schedule which is built as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de22c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class ScheduleState(NamedTuple):\n",
    "    step_size: float\n",
    "    do_sample: bool\n",
    "\n",
    "\n",
    "def build_schedule(\n",
    "    num_training_steps,\n",
    "    num_cycles=4,\n",
    "    initial_step_size=1e-3,\n",
    "    exploration_ratio=0.25,\n",
    "):\n",
    "    cycle_length = num_training_steps // num_cycles\n",
    "\n",
    "    def schedule_fn(step_id):\n",
    "        do_sample = False\n",
    "        if ((step_id % cycle_length)/cycle_length) >= exploration_ratio:\n",
    "            do_sample = True\n",
    "\n",
    "        cos_out = jnp.cos(jnp.pi * (step_id % cycle_length) / cycle_length) + 1\n",
    "        step_size = 0.5 * cos_out * initial_step_size\n",
    "\n",
    "        return ScheduleState(step_size, do_sample)\n",
    "\n",
    "    return schedule_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78138b36",
   "metadata": {},
   "source": [
    "Let us visualize a schedule for 200k training steps divided in 4 cycles. At each cycle 1/4th of the steps are dedicated to exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989d17c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "schedule_fn = build_schedule(20000, 4, 1e-1)\n",
    "schedule = [schedule_fn(i) for i in range(20000)]\n",
    "\n",
    "step_sizes = np.array([step.step_size for step in schedule])\n",
    "do_sample = np.array([step.do_sample for step in schedule])\n",
    "\n",
    "sampling_points = np.ma.masked_where(~do_sample, step_sizes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(step_sizes, lw=2, ls=\"--\", color=\"r\", label=\"Exploration stage\")\n",
    "ax.plot(sampling_points, lw=2, ls=\"-\", color=\"k\", label=\"Sampling stage\")\n",
    "\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "\n",
    "ax.set_xlabel(\"Training steps\", fontsize=20)\n",
    "ax.set_ylabel(\"Step size\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.title(\"Training schedule for Cyclical SGLD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa0ceb",
   "metadata": {},
   "source": [
    "### Step function\n",
    "\n",
    "Let us now build a step function for the Cyclical SGLD algorithm that can act as a drop-in replacement to the SGLD kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import blackjax\n",
    "import optax\n",
    "\n",
    "from blackjax.types import PyTree\n",
    "from optax._src.base import OptState\n",
    "\n",
    "\n",
    "class CyclicalSGMCMCState(NamedTuple):\n",
    "    \"\"\"State of the Cyclical SGMCMC sampler.\n",
    "    \"\"\"\n",
    "    position: PyTree\n",
    "    opt_state: OptState\n",
    "\n",
    "\n",
    "def cyclical_sgld(grad_estimator_fn, loglikelihood_fn):\n",
    "\n",
    "    # Initialize the SgLD step function\n",
    "    sgld = blackjax.sgld(grad_estimator_fn)\n",
    "    sgd = optax.sgd(1.)\n",
    "\n",
    "    def init_fn(position):\n",
    "        opt_state = sgd.init(position)\n",
    "        return CyclicalSGMCMCState(position, opt_state)\n",
    "\n",
    "    def step_fn(rng_key, state, minibatch, schedule_state):\n",
    "        \"\"\"Cyclical SGLD kernel.\"\"\"\n",
    "\n",
    "        def step_with_sgld(current_state):\n",
    "            rng_key, state, minibatch, step_size = current_state\n",
    "            new_position = sgld(rng_key, state.position, minibatch, step_size)\n",
    "            return CyclicalSGMCMCState(new_position, state.opt_state)\n",
    "\n",
    "        def step_with_sgd(current_state):\n",
    "            _, state, minibatch, step_size = current_state\n",
    "            grads = grad_estimator_fn(state.position, 0)\n",
    "            rescaled_grads = - 1. * step_size * grads\n",
    "            updates, new_opt_state = sgd.update(rescaled_grads, state.opt_state, state.position)\n",
    "            new_position = optax.apply_updates(state.position, updates)\n",
    "            return CyclicalSGMCMCState(new_position, new_opt_state)\n",
    "\n",
    "        new_state = jax.lax.cond(\n",
    "            schedule_state.do_sample,\n",
    "            step_with_sgld,\n",
    "            step_with_sgd,\n",
    "            (rng_key, state, minibatch, schedule_state.step_size)\n",
    "        )\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    return init_fn, step_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70feb5ef",
   "metadata": {},
   "source": [
    "We leave the implementation of Cyclical SGHMC as an exercise for the reader.\n",
    "\n",
    "```{note}\n",
    "If you want to Cyclical SGLD on one of your models you can simply copy and paste the code in the above cell and the cell where the schedule builder is implemented.\n",
    "```\n",
    "\n",
    "\n",
    "Let's sample using Cyclical SGLD, for the same number of iterations as with SGLD. We'll use 30 cycles, and sample 75% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca4daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "\n",
    "# 50k iterations\n",
    "# M = 30\n",
    "# initial step size = 0.09\n",
    "# ratio exploration = 1/4\n",
    "num_training_steps = 50000\n",
    "schedule_fn = build_schedule(num_training_steps, 30, 0.09, 0.25)\n",
    "schedule = [schedule_fn(i) for i in range(num_training_steps)]\n",
    "\n",
    "grad_fn = lambda x, _: jax.grad(logprob_fn)(x)\n",
    "init, step = cyclical_sgld(grad_fn, logprob_fn)\n",
    "\n",
    "rng_key = jax.random.PRNGKey(3)\n",
    "init_position = -10 + 20 * jax.random.uniform(rng_key, shape=(2,))\n",
    "init_state = init(init_position)\n",
    "\n",
    "\n",
    "state = init_state\n",
    "cyclical_samples = []\n",
    "for i in progress_bar(range(num_training_steps)):\n",
    "    _, rng_key = jax.random.split(rng_key)\n",
    "    state = jax.jit(step)(rng_key, state, 0, schedule[i])\n",
    "    if schedule[i].do_sample:\n",
    "        cyclical_samples.append(state.position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957e476",
   "metadata": {},
   "source": [
    "By looking at the trajectory of the sampler it seems that the distribution is much better explored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27a71c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = [sample[0] for sample in cyclical_samples]\n",
    "y = [sample[1] for sample in cyclical_samples]\n",
    "\n",
    "ax.plot(x, y, 'k-', lw=0.1, alpha=0.5)\n",
    "ax.set_xlim([-8, 8])\n",
    "ax.set_ylim([-8, 8])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title(\"Trajectory with Cyclical SGLD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630c64c",
   "metadata": {},
   "source": [
    "And the distribution indeed looks more better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e98262",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-stdout"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "xmin, ymin = -5, -5\n",
    "xmax, ymax = 5, 5\n",
    "\n",
    "nbins = 300j\n",
    "x = [sample[0] for sample in cyclical_samples]\n",
    "y = [sample[1] for sample in cyclical_samples]\n",
    "xx, yy = np.mgrid[xmin:xmax:nbins, ymin:ymax:nbins]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "values = np.vstack([x, y])\n",
    "kernel = gaussian_kde(values)\n",
    "f = np.reshape(kernel(positions).T, xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cfset = ax.contourf(xx, yy, f, cmap='Blues')\n",
    "ax.imshow(np.rot90(f), cmap='Blues', extent=[xmin, xmax, ymin, ymax])\n",
    "cset = ax.contour(xx, yy, f, colors='k')\n",
    "\n",
    "plt.rcParams['axes.titlepad'] = 15.\n",
    "plt.title(\"Samples from a mixture of 25 normal distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643b9d5",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "SciPy's `gaussian_kde` assumes that the total number of points provided is the sample size, it is thus not fit to use with raw MCMC samples. We should instead pass the bandwidth manually $n_{eff}^{-1/6}$ where $n_{eff}$ is the effective number of samples, and the figure should capture more modes.\n",
    "```\n",
    "\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   22,
   46,
   76,
   82,
   105,
   109,
   122,
   135,
   163,
   167,
   195,
   201,
   254,
   265,
   293,
   297,
   311,
   315,
   342
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}